<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<!-- %%% added by badri++  -->
<style type="text/css">
    /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
    a {
        color: #1772d0;
        text-decoration: none;
    }

    a:focus,
    a:hover {
        color: #f09228;
        text-decoration: none;
    }

    body,
    td,
    th {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 16px;
        font-weight: 400
    }

    heading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 17px;
        font-weight: 1000
    }

    strong {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 16px;
        font-weight: 800
    }

    strongred {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        color: 'red';
        font-size: 16px
    }

    sectionheading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 22px;
        font-weight: 600
    }

    pageheading {
        font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
        font-size: 38px;
        font-weight: 400
    }
</style>

<!-- %%% added by badri --  -->

<head>
    <meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link rel="stylesheet" href="css/jemdoc.css" type="text/css">
    <title>Badri Narayana Patro</title>
</head>

<body>
    <table summary="Table for page layout." id="tlayout">
        <tbody>
            <tr valign="top">
                <td id="layout-menu">
                    <!--<div class="menu-category"><a href="http://home.iitk.ac.in/%7Ebadri/index.html">Badri Patro</a></div>-->
                    <div class="menu-category">Badri Patro</div>
                    <div class="menu-item"><a href="index.html">Home</a></div>
                    <div class="menu-item"><a href="academics.html">Academics</a></div>
                    <div class="menu-item"><a href="experience.html">Experience</a></div>
                    <div class="menu-item"><a href="project.html">Projects</a></div>
                    <div class="menu-item"><a href="publication.html">Publication</a></div>
                    <div class="menu-item"><a href="research.html">Research</a></div>
                    <div class="menu-item"><a href="fun.html">Fun</a></div>
                    <!--<div class="menu-item"><a href="http://home.iitk.ac.in/%7Ebadri/courses.html">Courses</a></div>-->
                </td>
                <!--<div class="menu-item"><a href="http://home.iitk.ac.in/%7Ebadri/courses.html">Courses</a></div>-->
                </td>
                <td id="layout-content">
                    <div id="toptitle">
                        <h1>Publication</h1>
                    </div>

                    <h2>Conference Publication</h2>
                    <!-- <ul> -->
                      

                   <table width="80%" cellspacing="0" cellpadding="15" border="0" align="center">
                        <!-- actually width="100%" ></a> -->
                        <tbody>
                    
                            <tr>
                                <td width="33%" valign="top" align="center"><a
                                        href="https://delta-lab-iitk.github.io/TwoPlayer/"><img
                                            src="my_doc/image/cvpr_motivation_new.png" alt="sym" style="border-radius:15px"
                                            width="80%" height="25%"></a>
                                            <!-- actually width="100%" ></a> -->
                                </td>
                                <td width="67%" valign="top">
                                    <p><a href="https://delta-lab-iitk.github.io/TwoPlayer/" id="aaai2020">
                                            <!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
                                            <heading>Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA</heading>
                                        </a><br>
                                        <strong>Badri N. Patro,</strong>,Anupriy, Vinay P Namboodiri<br>
                                        <em> Association for the Advancement of Artificial Intelligence (AAAI)</em>, 2020
                                        <!-- <strong style="color:black">(Oral presentation)</strong> -->
                                    </p>
                    
                                    <div class="paper" id="assemblies19">
                                        <a href="https://delta-lab-iitk.github.io/TwoPlayer/">Project page</a> |
                                        <!-- <a href="https://arxiv.org/pdf/1904.01341.pdf">pdf</a> | -->
                                        <a href="javascript:toggleblock('assemblies19_abs')">abstract</a> |
                                        <a shape="rect" href="javascript:togglebib('assemblies19')" class="togglebib">bibtex</a> |
                                        <a href="https://arxiv.org/abs/1911.08618">arXiv</a> |
                                        <!-- <a href="http://home.iitk.ac.in/~vinodkk/idda_model/idda_ppt">ppt</a> | -->
                                        <!-- <a href="https://github.com/vinodkkurmi/DiscriminatorDomainAdaptation">code</a> -->
                                        <br>
                    
                                        <p align="justify"> <i id="assemblies19_abs" style="display: none;">In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to
                                        provide supervision for attention. An observation we make is that visual explanations as obtained through class
                                        activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a
                                        means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be
                                        suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to
                                        distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions
                                        as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual
                                        explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps
                                        that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention
                                        network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can
                                        also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with
                                        other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD)
                                        and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the
                                        attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of
                                        supervision.</i></p>
                    
                                        <pre xml:space="preserve" style="display: none;">@inproceedings{Patro2019ExplanationVA,
                                                title={Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA},
                                                author={Badri N. Patro and Anupriy and Vinay P. Namboodiri},
                                                year={2019}
                                                }
                                        </pre>
                                    </div>
                                </td>
                            </tr>
                    
                            <tr>
                                <td width="33%" valign="top" align="center"><a href="https://delta-lab-iitk.github.io/U-CAM"><img
                                            src="my_doc/image/iccv_intro_image.png" alt="sym" style="border-radius:15px"
                                            width="80%"  height="15%"></a>
                                            <!-- actually width="100%" ></a> -->
                                </td>
                                <td width="67%" valign="top">
                                    <p><a href="https://delta-lab-iitk.github.io/U-CAM" id="CVPR19">
                                            <!--       <img src="./Vinod_files/new.png"  width="6%" style="border-style: none"> -->
                                            <heading>U-CAM: Visual Explanation using Uncertainty based Class Activation Map</heading>
                                        </a><br>
                                        <strong>Badri N Patro</strong>, Mayank Lunayach, Shivansh Patel Vinay P. Namboodiri<br>
                                        <em>Proceedings of IEEE Conference on International Conference in Computer Vision (ICCV), Seoul, South Korea, </em>, 2019<br>
                                    </p>
                    
                                    <div class="paper" id="iclr19">
                                        <a href="https://delta-lab-iitk.github.io/U-CAM/">Project page</a> |
                                        <a
                                            href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Patro_U-CAM_Visual_Explanation_Using_Uncertainty_Based_Class_Activation_Maps_ICCV_2019_paper.pdf">pdf</a>
                                        |
                                        <a href="javascript:toggleblock('iclr19_abs')">abstract</a> |
                                        <a shape="rect" href="javascript:togglebib('iclr19')" class="togglebib">bibtex</a> |
                                        <a href="https://arxiv.org/abs/1908.06306">arXiv</a> |
                                        <!-- <a href="http://home.iitk.ac.in/~vinodkk/cada/CADA_CVPR2019.pdf">poster</a>| -->
                                        <!-- <a href="https://github.com/DelTA-Lab-IITK/CADA">code</a>| -->
                                        <!-- <a href="http://home.iitk.ac.in/~vinodkk/cada/Supplementary_cada.pdf">supplementary</a> -->
                                        <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
                    
                                        <p align="justify"> <i id="iclr19_abs" style="display: none;">Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains
                                        gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question
                                        answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients
                                        for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate
                                        better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of
                                        correlation with human attention regions. The improved attention maps result in consistent improvement for various
                                        methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining
                                        improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the
                                        visual question answering task on all standard benchmarks and comparison with state of the art methods.</i></p>
                    
                                        <pre xml:space="preserve" style="display: none;">@InProceedings{Patro_2019_ICCV,
                                        author = {Patro, Badri N. and Lunayach, Mayank and Patel, Shivansh and Namboodiri, Vinay P.},
                                        title = {U-CAM: Visual Explanation Using Uncertainty Based Class Activation Maps},
                                        booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
                                        month = {October},
                                        year = {2019}
                                        }
                                        </pre>
                                    </div>
                                </td>
                            </tr>
                    
                            <tr>
                                <td width="33%" valign="top" align="center"><a href="https://badripatro.github.io/MDN-VQG/"><img
                                            src="my_doc/image/emnlp18.png" alt="sym" style="border-style: none"
                                            width="80%"  height="15%" ></a>
                                            <!-- actually width="100%" ></a> -->
                                </td>
                                <td width="67%" valign="top">
                                    <p><a href="https://badripatro.github.io/MDN-VQG/" id="emnlp18">
                                            <heading>Multimodal Differential Network for Visual Question Generation</heading>
                                        </a><br>
                                        <strong>Badri N Patro</strong>, Sandeep Kumar, Vinod K Kurmi, Vinay P Namboodiri<br>
                                        <em>Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2018<br>
                                        <!--  <em><a href="https://sites.google.com/view/cvpr2018-robotic-vision/home" style="color:black" target="_blank">Deep Learning in Robotics Vision Workshop</a> (CVPR)</em>, 2018 (Oral)<br> -->
                                    </p>
                    
                                    <div class="paper" id="emnlp18">
                                        <a href="https://badripatro.github.io/MDN-VQG/">Project page</a> |
                                        <a href="https://arxiv.org/pdf/1808.03986.pdf">pdf</a> |
                                        <a href="javascript:toggleblock('emnlp18_abs')">abstract</a> |
                                        <a shape="rect" href="javascript:togglebib('cvprw18')" class="togglebib">bibtex</a> |
                                        <a href="https://arxiv.org/pdf/1808.03986.pdf">arXiv</a> |
                                        <a href="https://github.com/vinodkkurmi/MDN-VQG/">code</a>
                                        <br>
                    
                                        <p align="justify"> <i id="emnlp18_abs" style="display: none;">Generating
                                                natural questions from an image is a semantic task that requires using
                                                visual and language modality to learn multimodal representations. Images
                                                can have multiple visual and language contexts that are relevant for
                                                generating questions namely places, captions, and tags. In this paper,
                                                we propose the use of exemplars for obtaining the relevant context. We
                                                obtain this by using a Multimodal Differential Network to produce
                                                natural and engaging questions. The generated questions show a
                                                remarkable similarity to the natural questions as validated by a human
                                                study. Further, we observe that the proposed approach substantially
                                                improves over state-of-the-art benchmarks on the quantitative metrics
                                                (BLEU, METEOR, ROUGE, and CIDEr).</i></p>
                    
                                        <pre xml:space="preserve" style="display: none;">@inproceedings{patro2018multimodal,
                                            title={Multimodal Differential Network for Visual Question Generation},
                                            author={Patro, Badri Narayana and Kumar, Sandeep and Kurmi, Vinod Kumar and Namboodiri, Vinay},
                                            booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
                                            pages={4002--4012},
                                            year={2018}
                                            }
                                         </pre>
                                    </div>
                                </td>
                            </tr>
                    
                            <tr>
                                <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1807.07560"><img
                                            src="my_doc/image/coling18.png" alt="sym" style="border-radius:15px"
                                            width="80%"  height="25%" ></a>
                                            <!-- actually width="100%" ></a> -->
                                </td>
                                <td width="67%" valign="top">
                                    <p><a href="https://arxiv.org/abs/1807.07560" id="CompGAN18">
                                            <heading>Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator
                                            </heading>
                                        </a><br>
                                        <strong>Badri N Patro*</strong>,Vinod K Kurmi*, Sandeep K*, Vinay P Namboodiri&nbsp;(*equal
                                        contribution)<br>
                                        <em>International Conference on Computational Linguistics (COLING)</em>, 2018<br>
                                    </p>
                    
                                    <div class="paper" id="compgan18">
                                        <a href="https://github.com/vinodkkurmi/PQG/">Project page</a> |
                                        <a href="https://www.aclweb.org/anthology/C18-1230">pdf</a> |
                                        <a href="javascript:toggleblock('cvprw18_abs')">abstract</a> |
                                        <a shape="rect" href="javascript:togglebib('cvprw18')" class="togglebib">bibtex</a> |
                                        <a href="https://arxiv.org/abs/1806.00807">arXiv</a> |
                                        <a href="https://github.com/vinodkkurmi/PQG/">code</a>
                                        <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
                                        <br>
                    
                                        <p align="justify"> <i id="compgan18_abs" style="display: none;">In
                                                this paper, we propose a method for obtaining sentence-level
                                                embeddings. While the problem of securing word-level embeddings is very
                                                well studied, we propose a novel method for obtaining sentence-level
                                                embeddings. This is obtained by a simple method in the context of
                                                solving the paraphrase generation task. If we use a sequential
                                                encoder-decoder model for generating paraphrase, we would like the
                                                generated paraphrase to be semantically close to the original sentence.
                                                One way to ensure this is by adding constraints for true paraphrase
                                                embeddings to be close and unrelated paraphrase candidate sentence
                                                embeddings to be far. This is ensured by using a sequential pair-wise
                                                discriminator that shares weights with the encoder that is trained with a
                                                suitable loss function. Our loss function penalizes paraphrase sentence
                                                embedding distances from being too large. This loss is used in
                                                combination with a sequential encoder-decoder network. We also validated
                                                our method by evaluating the obtained embeddings for a sentiment
                                                analysis task. The proposed method results in semantic embeddings and
                                                outperforms the state-of-the-art on the paraphrase generation and
                                                sentiment analysis task on standard datasets. These results are also
                                                shown to be statistically significant.</i></p>
                    
                                        <pre xml:space="preserve" style="display: none;">@inproceedings{patro2018learning,
                                                title={Learning Semantic Sentence Embeddings using Sequential Pair-wise Discriminator},
                                                author={Patro, Badri Narayana and Kurmi, Vinod Kumar and Kumar, Sandeep and Namboodiri, Vinay},
                                                booktitle={Proceedings of the 27th International Conference on Computational Linguistics},
                                                pages={2715--2729},
                                                year={2018}
                                                }
                    </pre>
                                    </div>
                                </td>
                            </tr>

                            <tr>
                                <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/1804.00298"><img
                                            src="my_doc/image/cvpr_2018_intro.png" alt="sym" style="border-radius:15px" width="80%" height="15%"></a>
                                            <!-- actually width="100%" ></a> -->
                                </td>
                                <td width="67%" valign="top">
                                    <p><a href="https://arxiv.org/abs/1804.00298" id="CompGAN18">
                                            <heading>Differential Attention for Visual Question Answering,
                                            </heading>
                                        </a><br>
                                        <strong>Badri N. Patro</strong>,Vinay P. Namboodiri <br>
                                        <em>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, Utah,USA,</em>2018<br>
                                    </p>
                            
                                    <div class="paper" id="compgan18">
                                        <a href="https://badripatro.github.io/DVQA/">Project page</a> |
                                        <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/3451.html">pdf</a> |
                                        <a href="javascript:toggleblock('cvprw18_abs')">abstract</a> |
                                        <a shape="rect" href="javascript:togglebib('cvprw18')" class="togglebib">bibtex</a> |
                                        <a href="https://arxiv.org/abs/1804.00298">arXiv</a> |
                                        <!-- <a href="https://github.com/vinodkkurmi/PQG/">code</a> -->
                                        <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
                                        <br>
                            
                                        <p align="justify"> <i id="compgan18_abs" style="display: none;">In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a
                                        number of images during training. A number of methods have focused on solving this problem by using image based
                                        attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when
                                        solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that
                                        humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using
                                        an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention
                                        region. This differential attention is closer to human attention than other image based attention methods. It also helps
                                        in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We
                                        perform better than other image based attention methods and are competitive with other state of the art methods that
                                        focus on both image and questions.</i></p>
                            
                                        <pre xml:space="preserve" style="display: none;">@InProceedings{Patro_2018_CVPR,
                                        author = {Patro, Badri and Namboodiri, Vinay P.},
                                        title = {Differential Attention for Visual Question Answering},
                                        booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
                                        month = {June},
                                        year = {2018}
                                        }
                                                </pre>
                                    </div>
                                </td>
                            </tr>

                    <tr>
                        <td width="33%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/6968360?arnumber=6968360&tag=1"><img
                                    src="my_doc/image/icacci_2014_intro.png" alt="sym" style="border-radius:15px" width="80%"  height="25%" ></a>  
                                     <!-- actually width="100%" ></a> -->
                        </td>
                        <td width="67%" valign="top">
                            <p><a href="https://ieeexplore.ieee.org/document/6968360?arnumber=6968360&tag=1" id="CompGAN18">
                                    <heading>Design and implementation of novel image segmentation and BLOB detection algorithm for real-time video surveillance
                                    using DaVinci processor,</heading>
                                </a><br>
                                <strong>Badri N. Patro</strong><br>
                                <em>Proceedings of IEEE Conference on International Conference on Advances in Computing,Communications and Informatics (ICACCI), pp. 1909-1915, India,Sept </em>2014<br>
                            </p>
                    
                            <div class="paper" id="compgan18">
                                <!-- <a href="https://badripatro.github.io/DVQA/">Project page</a> | -->
                                <a href="https://ieeexplore.ieee.org/document/6968360?arnumber=6968360&tag=1">pdf</a> |
                                <a href="javascript:toggleblock('cvprw18_abs')">abstract</a> |
                                <a shape="rect" href="javascript:togglebib('cvprw18')" class="togglebib">bibtex</a> |
                                <a href="my_doc/doc/icacci.pdf">arXiv</a> |
                                <!-- <a href="https://github.com/vinodkkurmi/PQG/">code</a> -->
                                <!-- <a href="https://pathak22.github.io/large-scale-curiosity/index.html#sourceCode">code</a> -->
                                <br>
                    
                                <p align="justify"> <i id="compgan18_abs" style="display: none;">A video surveillance system is primarily designed to track key objects, or people exhibiting suspicious behavior, as
                                they move from one position to another and record it for possible future use. The critical parts of an object tracking
                                algorithm are object segmentation, image clusters detection, and identification and tracking of these image clusters.
                                The major roadblocks of the tracking algorithm arise due to abrupt object shape, ambiguity in number and size of
                                objects, background and illumination changes, noise in images, contour sliding, occlusions and real-time processing.
                                This paper will explain a solution of the object tracking problem, in 3 stages: In the first stage, design a novel
                                object segmentation and background subtraction algorithm, These algorithm will take care of salt pepper noise, and
                                changes in scene illumination. In the second stage, solve the abrupt object shape problems, objects size and count
                                various objects present , using image clusters detected and identified by the BLOBs (Binary Large OBjects) in the image
                                frame. In the third stage, design a centroid based tracking method, to improve robustness w.r.t occlusion and contour
                                sliding. A variety of optimizations, both at algorithm level and code level, are applied to the video surveillance
                                algorithm. At code level optimization mechanisms significantly reduce memory access, memory occupancy and improved
                                operation execution speed. Object tracking happens in real-time consuming 30 frames per second(fps) and is robust to
                                occlusion, contour sliding, background and illumination changes. Execution time for different blocks of this object
                                tracking algorithm were estimated and the accuracy of the detection was verified using the debugger and the profiler,
                                which will provided by the TI(Texas Instrument) Code Composer Studio (CCS). We demonstrate that this algorithm, with
                                code and algorithm level optimization on TIs DaVinci multimedia processor (TMS320DM6437), provides at least two times
                                speedup and is able to track a moving object in real-time as compared to without optimization.</i></p>
                    
                                <pre xml:space="preserve" style="display: none;">@inproceedings{patro2014design,
                                        title={Design and implementation of novel image segmentation and BLOB detection algorithm for real-time video
                                        surveillance using DaVinci processor},
                                        author={Patro, Badri Narayana},
                                        booktitle={2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)},
                                        pages={1909--1915},
                                        year={2014},
                                        organization={IEEE}
                                        }
                                 </pre>
                            </div>
                        </td>
                    </tr>

                </tbody>
            </table>
                    
    <!-- </ul> -->
    <!-- -------------------------------------------------------------------------------------- -->
            <!-- <h2>Arxiv Publication</h2> -->
   

<!-- -------------------------------------------------------------------------------------- -->
                    <h2> Journals and Conference List </h2>

                    <ul>
                    <li>
                    <a href="https://aivenues.github.io/" target="_blank"> Please find list of venues for Journals and
                        Conferences.</a>
                    </li>
                    </ul>


                    <div id="footer">
                        <div id="footer-text">
                            Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
                        </div>
                    </div>
                </td>
            </tr>
        </tbody>
    </table>


</body>

</html>
